{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Session7_2.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVbdg0z4poCc"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 699,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yVpodAMpoCi"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 700,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gzBTWCh2lG1"
      },
      "source": [
        "**Finding Mean and Std of CIFAR 10 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vTaMCyhtdK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713a3626-a9dd-4b94-96a8-25efb07cdb61"
      },
      "source": [
        "import numpy as np\n",
        "# load data\n",
        "from torchvision import datasets\n",
        "# load the training data\n",
        "train_data = datasets.CIFAR10('./cifar10_data', train=True, download=True)\n",
        "# use np.concatenate to stick all the images together to form a 1600000 X 32 X 3 array\n",
        "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\n",
        "# print(x)\n",
        "print(x.shape)\n",
        "# calculate the mean and std along the (0, 1) axes\n",
        "train_mean = np.mean(x, axis=(0, 1))\n",
        "train_std = np.std(x, axis=(0, 1))\n",
        "# the the mean and std\n",
        "print(train_mean/255, train_std/255)"
      ],
      "execution_count": 701,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "(1600000, 32, 3)\n",
            "[0.49139968 0.48215841 0.44653091] [0.24703223 0.24348513 0.26158784]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kjt4e2TpoCk"
      },
      "source": [
        "**Data Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-dNP1jBPsMD",
        "outputId": "a3bbc83c-fd4e-4e6c-f6ff-513af7aaaaa8"
      },
      "source": [
        "!pip install albumentations==0.4.6\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "alb_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.4),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "    A.CoarseDropout(max_holes = 1, max_height=4, max_width=4, min_holes = 1, \n",
        "                    min_height=1, min_width=1, fill_value=[0.49139968, 0.48215841, 0.44653091]),\n",
        "    #A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(mean=(0.49139968, 0.48215841, 0.44653091), std=(0.24703223, 0.24348513, 0.26158784)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "#train_transform = A.Compose(\n",
        "#    [\n",
        "#        A.SmallestMaxSize(max_size=160),\n",
        "#        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "#        A.RandomCrop(height=128, width=128),\n",
        "#        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
        "#        A.RandomBrightnessContrast(p=0.5),\n",
        "#        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "#        ToTensorV2(),\n",
        "#    ]\n",
        "#)"
      ],
      "execution_count": 702,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec5p5bCOpxGL"
      },
      "source": [
        "class Cifar10SearchDataset(torchvision.datasets.CIFAR10):\n",
        "    def __init__(self, root=\"~/data/cifar10\", train=True, download=True, transform=None):\n",
        "        super().__init__(root=root, train=train, download=download, transform=transform)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.data[index], self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            transformed = self.transform(image=image)\n",
        "            image = transformed[\"image\"]\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 703,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Tjigo2poCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad48554-62f4-44e0-a96a-d6db0b5cd21a"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.49139968, 0.48215841, 0.44653091), (0.229, 0.224, 0.225))])\n",
        "\n",
        "trainset = Cifar10SearchDataset(transform=alb_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 704,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28OpHA_LpoCq"
      },
      "source": [
        "**Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jjLwLLQpoCr"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, stride=2, dilation=1)#16\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 112, 3, padding=1, stride=2, dilation=1)#8\n",
        "        self.bn2 = nn.BatchNorm2d(112)\n",
        "        self.depth_conv = nn.Conv2d(in_channels=112, out_channels=112, kernel_size=3, groups=112, padding=1, stride=1, dilation=1)#8\n",
        "        self.point_conv = nn.Conv2d(in_channels=112, out_channels=84, kernel_size=1, stride=1, dilation=1)#8\n",
        "\n",
        "        self.depthwise_separable_conv = torch.nn.Sequential(self.depth_conv, self.point_conv)\n",
        "        self.bn3 = nn.BatchNorm2d(84)\n",
        "        self.conv3 = nn.Conv2d(84, 128, 3, dilation=2)#4\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool1 = nn.AvgPool2d(4)#1\n",
        "        \n",
        "        #self.conv4 = nn.Conv2d(24, 32, 3)#18 =>1*1*32 =>18*18*n\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(F.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.bn2(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.bn3(F.relu(self.depthwise_separable_conv(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.bn4(F.relu(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x= self.pool1(x)\n",
        "\n",
        "        x = x.view(-1, 128)\n",
        "        x = self.fc1(x)\n",
        "        return x\n"
      ],
      "execution_count": 705,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21FaKOWUcFZG",
        "outputId": "06f2ab71-4391-4cff-fb56-96e6167e36ef"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "execution_count": 706,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "           Dropout-3           [-1, 64, 16, 16]               0\n",
            "            Conv2d-4            [-1, 112, 8, 8]          64,624\n",
            "       BatchNorm2d-5            [-1, 112, 8, 8]             224\n",
            "           Dropout-6            [-1, 112, 8, 8]               0\n",
            "            Conv2d-7            [-1, 112, 8, 8]           1,120\n",
            "            Conv2d-8            [-1, 112, 8, 8]           1,120\n",
            "            Conv2d-9             [-1, 84, 8, 8]           9,492\n",
            "           Conv2d-10             [-1, 84, 8, 8]           9,492\n",
            "      BatchNorm2d-11             [-1, 84, 8, 8]             168\n",
            "          Dropout-12             [-1, 84, 8, 8]               0\n",
            "           Conv2d-13            [-1, 128, 4, 4]          96,896\n",
            "      BatchNorm2d-14            [-1, 128, 4, 4]             256\n",
            "          Dropout-15            [-1, 128, 4, 4]               0\n",
            "        AvgPool2d-16            [-1, 128, 1, 1]               0\n",
            "           Linear-17                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 186,602\n",
            "Trainable params: 186,602\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.86\n",
            "Params size (MB): 0.71\n",
            "Estimated Total Size (MB): 1.58\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clMNB917-r6P"
      },
      "source": [
        "net = Net().to(device)"
      ],
      "execution_count": 707,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIKhAzvOF5z7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX3ce0aMLKMp",
        "outputId": "ae6f72e8-b56e-45c2-ede9-dd2e5a1f1e82"
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n"
      ],
      "execution_count": 708,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-odOrhdpoC2"
      },
      "source": [
        "**Train the network**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4xBbDROpoCv"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.02, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)"
      ],
      "execution_count": 709,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkuRfo1IpoC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5a5c31-3568-4f90-f126-b52efd0f6f17"
      },
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            if( (running_loss/2000) < 0.012):\n",
        "              scheduler.step()\n",
        "              scheduler.step()\n",
        "              print(\"Lr decr 2\")\n",
        "            elif( (running_loss/2000) < 0.017):\n",
        "              scheduler.step()\n",
        "              print(\"Lr decr 1\")\n",
        "            \n",
        "            running_loss = 0.0\n",
        "               \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 722,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 0.01182\n",
            "Lr decr 2\n",
            "[1,   200] loss: 0.01147\n",
            "Lr decr 2\n",
            "[1,   300] loss: 0.01176\n",
            "Lr decr 2\n",
            "[2,   100] loss: 0.01145\n",
            "Lr decr 2\n",
            "[2,   200] loss: 0.01204\n",
            "Lr decr 1\n",
            "[2,   300] loss: 0.01176\n",
            "Lr decr 2\n",
            "[3,   100] loss: 0.01149\n",
            "Lr decr 2\n",
            "[3,   200] loss: 0.01198\n",
            "Lr decr 2\n",
            "[3,   300] loss: 0.01183\n",
            "Lr decr 2\n",
            "[4,   100] loss: 0.01176\n",
            "Lr decr 2\n",
            "[4,   200] loss: 0.01205\n",
            "Lr decr 1\n",
            "[4,   300] loss: 0.01167\n",
            "Lr decr 2\n",
            "[5,   100] loss: 0.01212\n",
            "Lr decr 1\n",
            "[5,   200] loss: 0.01174\n",
            "Lr decr 2\n",
            "[5,   300] loss: 0.01178\n",
            "Lr decr 2\n",
            "[6,   100] loss: 0.01202\n",
            "Lr decr 1\n",
            "[6,   200] loss: 0.01170\n",
            "Lr decr 2\n",
            "[6,   300] loss: 0.01229\n",
            "Lr decr 1\n",
            "[7,   100] loss: 0.01176\n",
            "Lr decr 2\n",
            "[7,   200] loss: 0.01087\n",
            "Lr decr 2\n",
            "[7,   300] loss: 0.01174\n",
            "Lr decr 2\n",
            "[8,   100] loss: 0.01194\n",
            "Lr decr 2\n",
            "[8,   200] loss: 0.01176\n",
            "Lr decr 2\n",
            "[8,   300] loss: 0.01140\n",
            "Lr decr 2\n",
            "[9,   100] loss: 0.01152\n",
            "Lr decr 2\n",
            "[9,   200] loss: 0.01166\n",
            "Lr decr 2\n",
            "[9,   300] loss: 0.01197\n",
            "Lr decr 2\n",
            "[10,   100] loss: 0.01153\n",
            "Lr decr 2\n",
            "[10,   200] loss: 0.01214\n",
            "Lr decr 1\n",
            "[10,   300] loss: 0.01151\n",
            "Lr decr 2\n",
            "[11,   100] loss: 0.01172\n",
            "Lr decr 2\n",
            "[11,   200] loss: 0.01191\n",
            "Lr decr 2\n",
            "[11,   300] loss: 0.01182\n",
            "Lr decr 2\n",
            "[12,   100] loss: 0.01190\n",
            "Lr decr 2\n",
            "[12,   200] loss: 0.01188\n",
            "Lr decr 2\n",
            "[12,   300] loss: 0.01176\n",
            "Lr decr 2\n",
            "[13,   100] loss: 0.01181\n",
            "Lr decr 2\n",
            "[13,   200] loss: 0.01163\n",
            "Lr decr 2\n",
            "[13,   300] loss: 0.01184\n",
            "Lr decr 2\n",
            "[14,   100] loss: 0.01175\n",
            "Lr decr 2\n",
            "[14,   200] loss: 0.01185\n",
            "Lr decr 2\n",
            "[14,   300] loss: 0.01166\n",
            "Lr decr 2\n",
            "[15,   100] loss: 0.01146\n",
            "Lr decr 2\n",
            "[15,   200] loss: 0.01183\n",
            "Lr decr 2\n",
            "[15,   300] loss: 0.01214\n",
            "Lr decr 1\n",
            "[16,   100] loss: 0.01190\n",
            "Lr decr 2\n",
            "[16,   200] loss: 0.01179\n",
            "Lr decr 2\n",
            "[16,   300] loss: 0.01185\n",
            "Lr decr 2\n",
            "[17,   100] loss: 0.01201\n",
            "Lr decr 1\n",
            "[17,   200] loss: 0.01174\n",
            "Lr decr 2\n",
            "[17,   300] loss: 0.01220\n",
            "Lr decr 1\n",
            "[18,   100] loss: 0.01168\n",
            "Lr decr 2\n",
            "[18,   200] loss: 0.01179\n",
            "Lr decr 2\n",
            "[18,   300] loss: 0.01135\n",
            "Lr decr 2\n",
            "[19,   100] loss: 0.01183\n",
            "Lr decr 2\n",
            "[19,   200] loss: 0.01164\n",
            "Lr decr 2\n",
            "[19,   300] loss: 0.01175\n",
            "Lr decr 2\n",
            "[20,   100] loss: 0.01133\n",
            "Lr decr 2\n",
            "[20,   200] loss: 0.01230\n",
            "Lr decr 1\n",
            "[20,   300] loss: 0.01171\n",
            "Lr decr 2\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLrvL_fHpoC5"
      },
      "source": [
        "**Test the network**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ4-5CESpoDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa687a8e-fdf8-4730-956b-07cf4d8376e3"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device) \n",
        "        outputs = net(images.to(device))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 727,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 83.59 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDNFuDEDpoDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3aa61a6-cedb-44f4-f098-5876234c0d84"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 712,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 79 %\n",
            "Accuracy of   car : 92 %\n",
            "Accuracy of  bird : 60 %\n",
            "Accuracy of   cat : 47 %\n",
            "Accuracy of  deer : 77 %\n",
            "Accuracy of   dog : 75 %\n",
            "Accuracy of  frog : 86 %\n",
            "Accuracy of horse : 80 %\n",
            "Accuracy of  ship : 90 %\n",
            "Accuracy of truck : 87 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}